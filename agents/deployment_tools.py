"""
éƒ¨ç½²ä¸“ç”¨å·¥å…·é›†
åŒ…å«Dockerå®¹å™¨åŒ–ã€CI/CDæµæ°´çº¿ã€ç¯å¢ƒç®¡ç†ã€ç›‘æ§ç­‰éƒ¨ç½²ç›¸å…³å·¥å…·
"""

import os
import yaml
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from crewai_tools import BaseTool


@dataclass
class DeploymentConfig:
    """éƒ¨ç½²é…ç½®æ•°æ®ç±»"""
    environment: str
    services: List[str]
    ports: Dict[str, int]
    volumes: Dict[str, str]
    env_vars: Dict[str, str]
    health_checks: Dict[str, str]


class DockerManagementTool(BaseTool):
    """Dockerå®¹å™¨ç®¡ç†å·¥å…·"""
    name: str = "Docker Management Tool"
    description: str = """
    Dockerå®¹å™¨åŒ–å’Œç®¡ç†å·¥å…·ï¼Œæ”¯æŒï¼š
    - Dockerfileç”Ÿæˆå’Œä¼˜åŒ–
    - Docker Composeé…ç½®ç®¡ç†
    - å®¹å™¨æ„å»ºã€è¿è¡Œã€ç›‘æ§
    - é•œåƒç®¡ç†å’Œä¼˜åŒ–
    - å¤šç¯å¢ƒå®¹å™¨ç¼–æ’
    """

    def _run(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒDockerç®¡ç†æ“ä½œ"""
        try:
            if action == "generate_dockerfile":
                return self._generate_dockerfile(**kwargs)
            elif action == "generate_compose":
                return self._generate_docker_compose(**kwargs)
            elif action == "build_images":
                return self._build_docker_images(**kwargs)
            elif action == "manage_containers":
                return self._manage_containers(**kwargs)
            elif action == "optimize_images":
                return self._optimize_docker_images(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„Dockeræ“ä½œ: {action}"
        except Exception as e:
            return f"âŒ Dockeræ“ä½œå¤±è´¥: {str(e)}"

    def _generate_dockerfile(self, service_type: str, base_image: str = "", 
                           requirements: List[str] = None, **kwargs) -> str:
        """ç”Ÿæˆä¼˜åŒ–çš„Dockerfile"""
        if service_type == "fastapi":
            dockerfile_content = f"""# FastAPI Production Dockerfile
FROM python:3.11-slim as base

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶å¹¶å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \\
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY ./backend /app/backend

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app && \\
    chown -R app:app /app
USER app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
"""
        elif service_type == "vue":
            dockerfile_content = f"""# Vue.js Multi-stage Dockerfile
FROM node:18-alpine as build

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶packageæ–‡ä»¶
COPY frontend/package*.json ./

# å®‰è£…ä¾èµ–
RUN npm ci --only=production

# å¤åˆ¶æºä»£ç 
COPY frontend/ .

# æ„å»ºåº”ç”¨
RUN npm run build

# ç”Ÿäº§é˜¶æ®µ
FROM nginx:alpine as production

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=build /app/dist /usr/share/nginx/html

# å¤åˆ¶nginxé…ç½®
COPY nginx.conf /etc/nginx/nginx.conf

# æš´éœ²ç«¯å£
EXPOSE 80

# å¯åŠ¨nginx
CMD ["nginx", "-g", "daemon off;"]
"""
        else:
            dockerfile_content = f"""# Generic Service Dockerfile
FROM {base_image or 'alpine:latest'}

WORKDIR /app

# æ·»åŠ è‡ªå®šä¹‰é…ç½®
{chr(10).join(requirements or [])}

CMD ["echo", "Service ready"]
"""

        return f"âœ… ç”Ÿæˆ {service_type} Dockerfile:\n\n```dockerfile\n{dockerfile_content}\n```"

    def _generate_docker_compose(self, environment: str, services: List[str], 
                                config: Dict[str, Any] = None) -> str:
        """ç”ŸæˆDocker Composeé…ç½®"""
        config = config or {}
        
        compose_config = {
            'version': '3.8',
            'services': {},
            'networks': {
                'app-network': {
                    'driver': 'bridge'
                }
            },
            'volumes': {
                'postgres_data': {},
                'redis_data': {}
            }
        }

        # æ ¹æ®æœåŠ¡åˆ—è¡¨ç”ŸæˆæœåŠ¡é…ç½®
        for service in services:
            if service == "backend":
                compose_config['services']['backend'] = {
                    'build': {
                        'context': '.',
                        'dockerfile': 'Dockerfile.backend'
                    },
                    'ports': ['8000:8000'],
                    'environment': [
                        f'ENV={environment}',
                        'DATABASE_URL=postgresql://postgres:postgres@db:5432/claude_fastapi',
                        'REDIS_URL=redis://redis:6379/0'
                    ],
                    'depends_on': ['db', 'redis'],
                    'networks': ['app-network'],
                    'healthcheck': {
                        'test': ['CMD', 'curl', '-f', 'http://localhost:8000/health'],
                        'interval': '30s',
                        'timeout': '10s',
                        'retries': 3
                    }
                }
            
            elif service == "frontend":
                compose_config['services']['frontend'] = {
                    'build': {
                        'context': '.',
                        'dockerfile': 'Dockerfile.frontend'
                    },
                    'ports': ['3000:80'],
                    'depends_on': ['backend'],
                    'networks': ['app-network']
                }
            
            elif service == "db":
                compose_config['services']['db'] = {
                    'image': 'postgres:15-alpine',
                    'environment': [
                        'POSTGRES_DB=claude_fastapi',
                        'POSTGRES_USER=postgres',
                        'POSTGRES_PASSWORD=postgres'
                    ],
                    'ports': ['5433:5432'],
                    'volumes': ['postgres_data:/var/lib/postgresql/data'],
                    'networks': ['app-network']
                }
            
            elif service == "redis":
                compose_config['services']['redis'] = {
                    'image': 'redis:7-alpine',
                    'ports': ['6379:6379'],
                    'volumes': ['redis_data:/data'],
                    'networks': ['app-network']
                }

        yaml_content = yaml.dump(compose_config, default_flow_style=False, sort_keys=False)
        return f"âœ… ç”Ÿæˆ {environment} Docker Compose é…ç½®:\n\n```yaml\n{yaml_content}\n```"

    def _build_docker_images(self, services: List[str], environment: str = "development") -> str:
        """æ„å»ºDockeré•œåƒ"""
        results = []
        
        for service in services:
            try:
                # æ¨¡æ‹Ÿæ„å»ºè¿‡ç¨‹
                build_cmd = f"docker build -t claude-fastapi-{service}:{environment} -f Dockerfile.{service} ."
                results.append(f"âœ… æ„å»º {service} é•œåƒ: {build_cmd}")
            except Exception as e:
                results.append(f"âŒ æ„å»º {service} å¤±è´¥: {str(e)}")
        
        return "\n".join(results)

    def _manage_containers(self, action: str, services: List[str] = None) -> str:
        """ç®¡ç†å®¹å™¨æ“ä½œ"""
        services = services or []
        results = []
        
        if action == "start":
            results.append("ğŸš€ å¯åŠ¨å®¹å™¨æœåŠ¡:")
            for service in services:
                results.append(f"  âœ… å¯åŠ¨ {service}: docker-compose up -d {service}")
        
        elif action == "stop":
            results.append("ğŸ›‘ åœæ­¢å®¹å™¨æœåŠ¡:")
            for service in services:
                results.append(f"  âœ… åœæ­¢ {service}: docker-compose stop {service}")
        
        elif action == "restart":
            results.append("ğŸ”„ é‡å¯å®¹å™¨æœåŠ¡:")
            for service in services:
                results.append(f"  âœ… é‡å¯ {service}: docker-compose restart {service}")
        
        elif action == "logs":
            results.append("ğŸ“‹ æŸ¥çœ‹å®¹å™¨æ—¥å¿—:")
            for service in services:
                results.append(f"  ğŸ“ {service} æ—¥å¿—: docker-compose logs -f {service}")
        
        return "\n".join(results)

    def _optimize_docker_images(self, services: List[str]) -> str:
        """ä¼˜åŒ–Dockeré•œåƒ"""
        optimizations = [
            "ğŸ¯ Dockeré•œåƒä¼˜åŒ–å»ºè®®:",
            "",
            "1. å¤šé˜¶æ®µæ„å»º:",
            "   - ä½¿ç”¨ multi-stage builds å‡å°‘é•œåƒå¤§å°",
            "   - åˆ†ç¦»æ„å»ºç¯å¢ƒå’Œè¿è¡Œç¯å¢ƒ",
            "",
            "2. åŸºç¡€é•œåƒä¼˜åŒ–:",
            "   - ä½¿ç”¨ Alpine Linux è½»é‡çº§é•œåƒ",
            "   - é€‰æ‹©åˆé€‚çš„ Python/Node.js ç‰ˆæœ¬",
            "",
            "3. å±‚ç¼“å­˜ä¼˜åŒ–:",
            "   - ä¼˜åŒ– COPY æŒ‡ä»¤é¡ºåº",
            "   - åˆå¹¶ RUN æŒ‡ä»¤å‡å°‘å±‚æ•°",
            "",
            "4. å®‰å…¨åŠ å›º:",
            "   - ä½¿ç”¨érootç”¨æˆ·è¿è¡Œåº”ç”¨",
            "   - ç§»é™¤ä¸å¿…è¦çš„åŒ…å’Œæ–‡ä»¶",
            "",
            "5. å¥åº·æ£€æŸ¥:",
            "   - æ·»åŠ  HEALTHCHECK æŒ‡ä»¤",
            "   - é…ç½®åˆé€‚çš„æ£€æŸ¥é—´éš”"
        ]
        
        return "\n".join(optimizations)


class CICDPipelineTool(BaseTool):
    """CI/CDæµæ°´çº¿ç®¡ç†å·¥å…·"""
    name: str = "CI/CD Pipeline Tool"
    description: str = """
    CI/CDæµæ°´çº¿è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ”¯æŒï¼š
    - GitHub Actionså·¥ä½œæµç”Ÿæˆ
    - è‡ªåŠ¨åŒ–æµ‹è¯•å’Œéƒ¨ç½²æµç¨‹
    - å¤šç¯å¢ƒéƒ¨ç½²ç­–ç•¥
    - ä»£ç è´¨é‡æ£€æŸ¥é›†æˆ
    - å®¹å™¨é•œåƒæ„å»ºå’Œæ¨é€
    """

    def _run(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒCI/CDç®¡é“æ“ä½œ"""
        try:
            if action == "generate_workflow":
                return self._generate_github_workflow(**kwargs)
            elif action == "setup_pipeline":
                return self._setup_cicd_pipeline(**kwargs)
            elif action == "deploy_strategy":
                return self._create_deployment_strategy(**kwargs)
            elif action == "quality_gates":
                return self._setup_quality_gates(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„CI/CDæ“ä½œ: {action}"
        except Exception as e:
            return f"âŒ CI/CDæ“ä½œå¤±è´¥: {str(e)}"

    def _generate_github_workflow(self, workflow_type: str, environments: List[str] = None) -> str:
        """ç”ŸæˆGitHub Actionså·¥ä½œæµ"""
        environments = environments or ["development", "production"]
        
        if workflow_type == "full_cicd":
            workflow_content = """name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
        node-version: [18]
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r backend/requirements-dev.txt
    
    - name: Install Node.js dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run backend tests
      run: |
        cd backend
        pytest tests/ -v --cov=. --cov-report=xml
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:unit
    
    - name: Lint code
      run: |
        cd backend && flake8 .
        cd frontend && npm run lint

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push Backend image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.backend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-backend:latest
    
    - name: Build and push Frontend image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.frontend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-frontend:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
        # è¿™é‡Œæ·»åŠ å…·ä½“çš„éƒ¨ç½²è„šæœ¬
"""
        elif workflow_type == "test_only":
            workflow_content = """name: Test Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        cd backend
        pytest tests/ -v
"""
        else:
            workflow_content = f"# {workflow_type} å·¥ä½œæµé…ç½®\n# è¯·æä¾›å…·ä½“çš„å·¥ä½œæµç±»å‹"

        return f"âœ… ç”Ÿæˆ {workflow_type} GitHub Actions å·¥ä½œæµ:\n\n```yaml\n{workflow_content}\n```"

    def _setup_cicd_pipeline(self, platform: str, features: List[str]) -> str:
        """è®¾ç½®CI/CDæµæ°´çº¿"""
        pipeline_features = {
            "automated_testing": "è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œ",
            "code_quality": "ä»£ç è´¨é‡æ£€æŸ¥",
            "security_scan": "å®‰å…¨æ¼æ´æ‰«æ", 
            "docker_build": "Dockeré•œåƒæ„å»º",
            "multi_env_deploy": "å¤šç¯å¢ƒéƒ¨ç½²",
            "rollback": "è‡ªåŠ¨å›æ»šæœºåˆ¶",
            "monitoring": "éƒ¨ç½²ç›‘æ§å‘Šè­¦"
        }
        
        setup_steps = [
            f"ğŸ”§ è®¾ç½® {platform} CI/CDæµæ°´çº¿:",
            "",
            "ğŸ“‹ å¯ç”¨çš„åŠŸèƒ½:"
        ]
        
        for feature in features:
            if feature in pipeline_features:
                setup_steps.append(f"  âœ… {pipeline_features[feature]}")
        
        setup_steps.extend([
            "",
            "âš™ï¸ é…ç½®æ­¥éª¤:",
            "1. é…ç½®ç¯å¢ƒå˜é‡å’Œå¯†é’¥",
            "2. è®¾ç½®æ„å»ºå’Œæµ‹è¯•ç¯å¢ƒ",
            "3. é…ç½®éƒ¨ç½²ç›®æ ‡ç¯å¢ƒ",
            "4. è®¾ç½®ç›‘æ§å’Œå‘Šè­¦",
            "5. é…ç½®æƒé™å’Œè®¿é—®æ§åˆ¶"
        ])
        
        return "\n".join(setup_steps)

    def _create_deployment_strategy(self, strategy_type: str, environments: List[str]) -> str:
        """åˆ›å»ºéƒ¨ç½²ç­–ç•¥"""
        strategies = {
            "blue_green": """
ğŸ”µğŸŸ¢ è“ç»¿éƒ¨ç½²ç­–ç•¥:

1. å‡†å¤‡é˜¶æ®µ:
   - åœ¨ç»¿è‰²ç¯å¢ƒéƒ¨ç½²æ–°ç‰ˆæœ¬
   - è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
   - éªŒè¯æœåŠ¡å¥åº·çŠ¶æ€

2. åˆ‡æ¢é˜¶æ®µ:
   - å°†æµé‡ä»è“è‰²ç¯å¢ƒåˆ‡æ¢åˆ°ç»¿è‰²ç¯å¢ƒ
   - ç›‘æ§å…³é”®æŒ‡æ ‡å’Œé”™è¯¯ç‡
   - ä¿æŒè“è‰²ç¯å¢ƒä½œä¸ºå¿«é€Ÿå›æ»šå¤‡é€‰

3. æ¸…ç†é˜¶æ®µ:
   - ç¡®è®¤æ–°ç‰ˆæœ¬ç¨³å®šè¿è¡Œ
   - æ¸…ç†æ—§çš„è“è‰²ç¯å¢ƒ
   - æ›´æ–°ç¯å¢ƒæ ‡è®°
""",
            "rolling": """
ğŸ”„ æ»šåŠ¨éƒ¨ç½²ç­–ç•¥:

1. é€æ­¥æ›´æ–°:
   - ä¾æ¬¡æ›´æ–°æ¯ä¸ªæœåŠ¡å®ä¾‹
   - åœ¨æ›´æ–°è¿‡ç¨‹ä¸­ä¿æŒæœåŠ¡å¯ç”¨
   - å®æ—¶ç›‘æ§éƒ¨ç½²çŠ¶æ€

2. å¥åº·æ£€æŸ¥:
   - ç¡®ä¿æ¯ä¸ªå®ä¾‹æ›´æ–°åæ­£å¸¸å·¥ä½œ
   - å¤±è´¥æ—¶è‡ªåŠ¨åœæ­¢éƒ¨ç½²
   - æä¾›è¯¦ç»†çš„éƒ¨ç½²è¿›åº¦æŠ¥å‘Š

3. å®Œæ•´éªŒè¯:
   - æ‰€æœ‰å®ä¾‹æ›´æ–°å®Œæˆåè¿›è¡Œå…¨é¢æµ‹è¯•
   - éªŒè¯é›†ç¾¤çº§åˆ«çš„åŠŸèƒ½
""",
            "canary": """
ğŸ¤ é‡‘ä¸é›€éƒ¨ç½²ç­–ç•¥:

1. å°è§„æ¨¡å‘å¸ƒ:
   - å°†æ–°ç‰ˆæœ¬éƒ¨ç½²åˆ°å°‘é‡å®ä¾‹(5-10%)
   - å¯¼å…¥å°‘é‡ç”Ÿäº§æµé‡è¿›è¡Œæµ‹è¯•
   - æ”¶é›†æ€§èƒ½å’Œé”™è¯¯æŒ‡æ ‡

2. æ¸è¿›æ¨å¹¿:
   - æ ¹æ®æŒ‡æ ‡é€æ­¥å¢åŠ æµé‡æ¯”ä¾‹
   - ç›‘æ§ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿç¨³å®šæ€§
   - éšæ—¶å‡†å¤‡å¿«é€Ÿå›æ»š

3. å…¨é‡å‘å¸ƒ:
   - ç¡®è®¤é‡‘ä¸é›€ç‰ˆæœ¬ç¨³å®šåå…¨é‡å‘å¸ƒ
   - æŒç»­ç›‘æ§å…³é”®ä¸šåŠ¡æŒ‡æ ‡
"""
        }
        
        strategy_content = strategies.get(strategy_type, f"âŒ ä¸æ”¯æŒçš„éƒ¨ç½²ç­–ç•¥: {strategy_type}")
        env_config = f"\nğŸ¯ ç›®æ ‡ç¯å¢ƒ: {', '.join(environments)}"
        
        return strategy_content + env_config

    def _setup_quality_gates(self, checks: List[str]) -> str:
        """è®¾ç½®è´¨é‡é—¨ç¦"""
        quality_checks = {
            "test_coverage": "æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥ (>80%)",
            "code_quality": "ä»£ç è´¨é‡æ‰«æ (SonarQube)",
            "security_scan": "å®‰å…¨æ¼æ´æ‰«æ",
            "performance_test": "æ€§èƒ½æµ‹è¯•éªŒè¯",
            "lint_check": "ä»£ç è§„èŒƒæ£€æŸ¥",
            "dependency_scan": "ä¾èµ–æ¼æ´æ‰«æ"
        }
        
        gate_config = [
            "ğŸšª è´¨é‡é—¨ç¦é…ç½®:",
            "",
            "ğŸ“Š å¯ç”¨çš„æ£€æŸ¥é¡¹:"
        ]
        
        for check in checks:
            if check in quality_checks:
                gate_config.append(f"  âœ… {quality_checks[check]}")
        
        gate_config.extend([
            "",
            "âš¡ é—¨ç¦ç­–ç•¥:",
            "- æ‰€æœ‰æ£€æŸ¥é¡¹å¿…é¡»é€šè¿‡æ‰èƒ½ç»§ç»­éƒ¨ç½²",
            "- å¤±è´¥æ—¶é˜»æ­¢éƒ¨ç½²å¹¶å‘é€é€šçŸ¥",
            "- æä¾›è¯¦ç»†çš„å¤±è´¥åŸå› å’Œä¿®å¤å»ºè®®",
            "- æ”¯æŒæ‰‹åŠ¨å®¡æ‰¹æœºåˆ¶"
        ])
        
        return "\n".join(gate_config)


class EnvironmentManagementTool(BaseTool):
    """ç¯å¢ƒç®¡ç†å·¥å…·"""
    name: str = "Environment Management Tool"
    description: str = """
    ç¯å¢ƒé…ç½®å’Œç®¡ç†å·¥å…·ï¼Œæ”¯æŒï¼š
    - å¤šç¯å¢ƒé…ç½®ç®¡ç†
    - ç¯å¢ƒå˜é‡å’Œå¯†é’¥ç®¡ç†
    - é…ç½®æ¨¡æ¿ç”Ÿæˆ
    - ç¯å¢ƒåŒæ­¥å’Œè¿ç§»
    - ç¯å¢ƒå¥åº·ç›‘æ§
    """

    def _run(self, action: str, **kwargs) -> str:
        """æ‰§è¡Œç¯å¢ƒç®¡ç†æ“ä½œ"""
        try:
            if action == "create_env_config":
                return self._create_environment_config(**kwargs)
            elif action == "manage_secrets":
                return self._manage_secrets(**kwargs)
            elif action == "sync_environments":
                return self._sync_environments(**kwargs)
            elif action == "validate_config":
                return self._validate_configuration(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„ç¯å¢ƒç®¡ç†æ“ä½œ: {action}"
        except Exception as e:
            return f"âŒ ç¯å¢ƒç®¡ç†æ“ä½œå¤±è´¥: {str(e)}"

    def _create_environment_config(self, environment: str, services: List[str], 
                                 config_type: str = "docker") -> str:
        """åˆ›å»ºç¯å¢ƒé…ç½®"""
        if config_type == "docker":
            config_content = f"""# {environment.upper()} Environment Configuration

# Database Configuration
DATABASE_URL=postgresql://postgres:postgres@db:5432/claude_fastapi_{environment}
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30

# Redis Configuration  
REDIS_URL=redis://redis:6379/0
REDIS_POOL_SIZE=10

# FastAPI Configuration
SECRET_KEY=your-secret-key-{environment}
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# CORS Configuration
ALLOWED_ORIGINS=http://localhost:3000,https://your-domain.com
ALLOWED_METHODS=GET,POST,PUT,DELETE,OPTIONS
ALLOWED_HEADERS=*

# Logging Configuration
LOG_LEVEL={'DEBUG' if environment == 'development' else 'INFO'}
LOG_FORMAT=json

# Monitoring Configuration
ENABLE_METRICS={"true" if environment == 'production' else 'false'}
METRICS_PORT=9090

# Environment Specific
ENV={environment}
DEBUG={'true' if environment == 'development' else 'false'}
"""
        elif config_type == "kubernetes":
            config_content = f"""apiVersion: v1
kind: ConfigMap
metadata:
  name: claude-fastapi-config-{environment}
  namespace: claude-fastapi-{environment}
data:
  ENV: "{environment}"
  LOG_LEVEL: "{'DEBUG' if environment == 'development' else 'INFO'}"
  DATABASE_POOL_SIZE: "20"
  REDIS_POOL_SIZE: "10"
  ACCESS_TOKEN_EXPIRE_MINUTES: "30"
  REFRESH_TOKEN_EXPIRE_DAYS: "7"
---
apiVersion: v1
kind: Secret
metadata:
  name: claude-fastapi-secrets-{environment}
  namespace: claude-fastapi-{environment}
type: Opaque
stringData:
  DATABASE_URL: "postgresql://postgres:password@db:5432/claude_fastapi_{environment}"
  REDIS_URL: "redis://redis:6379/0"
  SECRET_KEY: "your-secret-key-{environment}"
"""
        else:
            config_content = f"# {environment} ç¯å¢ƒé…ç½®\n# é…ç½®ç±»å‹: {config_type}"

        return f"âœ… ç”Ÿæˆ {environment} ç¯å¢ƒé…ç½® ({config_type}):\n\n```\n{config_content}\n```"

    def _manage_secrets(self, action: str, environment: str, secrets: Dict[str, str] = None) -> str:
        """ç®¡ç†ç¯å¢ƒå¯†é’¥"""
        secrets = secrets or {}
        
        if action == "generate":
            secret_template = f"""
ğŸ” {environment} ç¯å¢ƒå¯†é’¥ç®¡ç†:

âš ï¸ é‡è¦å¯†é’¥é…ç½®:
1. SECRET_KEY: JWTç­¾åå¯†é’¥ (è‡ªåŠ¨ç”Ÿæˆå¼ºå¯†é’¥)
2. DATABASE_PASSWORD: æ•°æ®åº“å¯†ç 
3. REDIS_PASSWORD: Rediså¯†ç  (å¯é€‰)
4. THIRD_PARTY_API_KEYS: ç¬¬ä¸‰æ–¹æœåŠ¡APIå¯†é’¥

ğŸ›¡ï¸ å¯†é’¥å®‰å…¨è¦æ±‚:
- ä½¿ç”¨å¼ºéšæœºå¯†é’¥ (è‡³å°‘32å­—ç¬¦)
- ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒå¯†é’¥
- å®šæœŸè½®æ¢é‡è¦å¯†é’¥
- ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ (å¦‚ HashiCorp Vault)

ğŸ“‹ å¯†é’¥å­˜å‚¨å»ºè®®:
- å¼€å‘ç¯å¢ƒ: .env.{environment} æ–‡ä»¶ (ä¸æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶)
- æµ‹è¯•ç¯å¢ƒ: CI/CDå¹³å°ç¯å¢ƒå˜é‡
- ç”Ÿäº§ç¯å¢ƒ: äº‘å¹³å°å¯†é’¥ç®¡ç†æœåŠ¡

ğŸ”„ å¯†é’¥è½®æ¢ç­–ç•¥:
- JWT SECRET_KEY: æ¯30å¤©
- æ•°æ®åº“å¯†ç : æ¯90å¤©
- APIå¯†é’¥: æ ¹æ®æœåŠ¡æä¾›å•†å»ºè®®
"""
        elif action == "validate":
            validation_results = []
            required_secrets = ["SECRET_KEY", "DATABASE_URL", "REDIS_URL"]
            
            for secret in required_secrets:
                if secret in secrets:
                    validation_results.append(f"  âœ… {secret}: å·²é…ç½®")
                else:
                    validation_results.append(f"  âŒ {secret}: ç¼ºå¤±")
            
            secret_template = f"""
ğŸ” {environment} å¯†é’¥éªŒè¯ç»“æœ:

{chr(10).join(validation_results)}

ğŸ’¡ ä¿®å¤å»ºè®®:
- ç¡®ä¿æ‰€æœ‰å¿…éœ€å¯†é’¥éƒ½å·²é…ç½®
- éªŒè¯å¯†é’¥æ ¼å¼å’Œæœ‰æ•ˆæ€§
- æ£€æŸ¥å¯†é’¥æƒé™å’Œè®¿é—®æ§åˆ¶
"""
        else:
            secret_template = f"âŒ ä¸æ”¯æŒçš„å¯†é’¥æ“ä½œ: {action}"

        return secret_template

    def _sync_environments(self, source_env: str, target_env: str, sync_type: str = "config") -> str:
        """åŒæ­¥ç¯å¢ƒé…ç½®"""
        sync_result = f"""
ğŸ”„ ç¯å¢ƒåŒæ­¥æ“ä½œ:

ğŸ“¤ æºç¯å¢ƒ: {source_env}
ğŸ“¥ ç›®æ ‡ç¯å¢ƒ: {target_env}
ğŸ¯ åŒæ­¥ç±»å‹: {sync_type}

âš¡ åŒæ­¥æ­¥éª¤:
1. å¤‡ä»½ç›®æ ‡ç¯å¢ƒå½“å‰é…ç½®
2. æ¯”è¾ƒæºç¯å¢ƒå’Œç›®æ ‡ç¯å¢ƒå·®å¼‚
3. åº”ç”¨é…ç½®æ›´æ”¹ (æ’é™¤ç¯å¢ƒç‰¹å®šé…ç½®)
4. éªŒè¯é…ç½®å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
5. é‡å¯ç›¸å…³æœåŠ¡ä½¿é…ç½®ç”Ÿæ•ˆ

âš ï¸ æ³¨æ„äº‹é¡¹:
- å¯†é’¥å’Œæ•æ„Ÿä¿¡æ¯ä¸ä¼šè‡ªåŠ¨åŒæ­¥
- ç¯å¢ƒç‰¹å®šçš„é…ç½®ä¼šè¢«ä¿ç•™
- åŒæ­¥å‰ä¼šåˆ›å»ºå®Œæ•´å¤‡ä»½
- æ”¯æŒå›æ»šåˆ°åŒæ­¥å‰çŠ¶æ€

ğŸ“‹ åŒæ­¥æ¸…å•:
  âœ… åº”ç”¨é…ç½®å‚æ•°
  âœ… æœåŠ¡ç«¯å£æ˜ å°„
  âœ… èµ„æºé™åˆ¶è®¾ç½®
  âš ï¸ å¯†é’¥ (éœ€è¦æ‰‹åŠ¨ç¡®è®¤)
  âš ï¸ ç¯å¢ƒç‰¹å®šURL (éœ€è¦æ‰‹åŠ¨è°ƒæ•´)
"""
        return sync_result

    def _validate_configuration(self, environment: str, config_data: Dict[str, Any] = None) -> str:
        """éªŒè¯ç¯å¢ƒé…ç½®"""
        config_data = config_data or {}
        
        validation_results = [
            f"ğŸ” {environment} ç¯å¢ƒé…ç½®éªŒè¯:",
            "",
            "ğŸ“‹ å¿…éœ€é…ç½®é¡¹æ£€æŸ¥:"
        ]
        
        required_configs = {
            "DATABASE_URL": "æ•°æ®åº“è¿æ¥URL",
            "REDIS_URL": "Redisè¿æ¥URL",
            "SECRET_KEY": "JWTç­¾åå¯†é’¥",
            "ALLOWED_ORIGINS": "CORSå…è®¸æ¥æº"
        }
        
        for key, description in required_configs.items():
            if key in config_data:
                validation_results.append(f"  âœ… {key}: {description}")
            else:
                validation_results.append(f"  âŒ {key}: {description} (ç¼ºå¤±)")
        
        validation_results.extend([
            "",
            "ğŸ”§ é…ç½®æ ¼å¼éªŒè¯:",
            "  âœ… ç¯å¢ƒå˜é‡å‘½åè§„èŒƒ",
            "  âœ… URLæ ¼å¼æ­£ç¡®æ€§",
            "  âœ… ç«¯å£å·æœ‰æ•ˆæ€§",
            "  âœ… å¸ƒå°”å€¼æ ¼å¼",
            "",
            "ğŸ’¡ ä¼˜åŒ–å»ºè®®:",
            "- ä½¿ç”¨ç¯å¢ƒç‰¹å®šçš„é…ç½®å€¼",
            "- å¯ç”¨é€‚å½“çš„æ—¥å¿—çº§åˆ«",
            "- é…ç½®åˆç†çš„èµ„æºé™åˆ¶",
            "- è®¾ç½®å¥åº·æ£€æŸ¥å‚æ•°"
        ])
        
        return "\n".join(validation_results)


class MonitoringSetupTool(BaseTool):
    """ç›‘æ§å’Œæ—¥å¿—ç®¡ç†å·¥å…·"""
    name: str = "Monitoring Setup Tool"
    description: str = """
    ç›‘æ§å’Œæ—¥å¿—ç®¡ç†å·¥å…·ï¼Œæ”¯æŒï¼š
    - åº”ç”¨æ€§èƒ½ç›‘æ§é…ç½®
    - æ—¥å¿—èšåˆå’Œåˆ†æ
    - å‘Šè­¦è§„åˆ™è®¾ç½®
    - å¥åº·æ£€æŸ¥é…ç½®
    - æŒ‡æ ‡ä»ªè¡¨æ¿åˆ›å»º
    """

    def _run(self, action: str, **kwargs) -> str:
        """æ‰§è¡Œç›‘æ§é…ç½®æ“ä½œ"""
        try:
            if action == "setup_monitoring":
                return self._setup_application_monitoring(**kwargs)
            elif action == "configure_logging":
                return self._configure_logging_system(**kwargs)
            elif action == "create_alerts":
                return self._create_alert_rules(**kwargs)
            elif action == "setup_dashboard":
                return self._setup_monitoring_dashboard(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„ç›‘æ§æ“ä½œ: {action}"
        except Exception as e:
            return f"âŒ ç›‘æ§é…ç½®å¤±è´¥: {str(e)}"

    def _setup_application_monitoring(self, services: List[str], monitoring_stack: str = "prometheus") -> str:
        """è®¾ç½®åº”ç”¨ç›‘æ§"""
        if monitoring_stack == "prometheus":
            config_content = """
ğŸ“Š Prometheus + Grafana ç›‘æ§æ ˆé…ç½®:

ğŸ¯ ç›‘æ§ç›®æ ‡:
- FastAPIåº”ç”¨æŒ‡æ ‡ (è¯·æ±‚é‡ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡)
- æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡ (è¿æ¥æ•°ã€æŸ¥è¯¢æ—¶é—´)
- Redisç¼“å­˜æŒ‡æ ‡ (å‘½ä¸­ç‡ã€å†…å­˜ä½¿ç”¨)
- ç³»ç»Ÿèµ„æºæŒ‡æ ‡ (CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ)

âš™ï¸ Prometheusé…ç½®:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'fastapi-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

ğŸ“ˆ å…³é”®æŒ‡æ ‡:
- http_requests_total: HTTPè¯·æ±‚æ€»æ•°
- http_request_duration_seconds: è¯·æ±‚å“åº”æ—¶é—´
- http_requests_in_progress: å¹¶å‘è¯·æ±‚æ•°
- database_connections_active: æ´»è·ƒæ•°æ®åº“è¿æ¥
- redis_memory_used_bytes: Rediså†…å­˜ä½¿ç”¨
"""
        else:
            config_content = f"ğŸ“Š {monitoring_stack} ç›‘æ§é…ç½® (å¾…å®ç°)"

        return config_content

    def _configure_logging_system(self, log_level: str, output_format: str = "json") -> str:
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging_config = f"""
ğŸ“ æ—¥å¿—ç³»ç»Ÿé…ç½®:

ğŸšï¸ æ—¥å¿—çº§åˆ«: {log_level}
ğŸ“„ è¾“å‡ºæ ¼å¼: {output_format}

âš™ï¸ FastAPIæ—¥å¿—é…ç½®:
```python
import logging
import sys
from loguru import logger

# ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.remove()

# æ·»åŠ æ§åˆ¶å°è¾“å‡º
logger.add(
    sys.stdout,
    format="{{time:YYYY-MM-DD HH:mm:ss}} | {{level}} | {{name}}:{{function}}:{{line}} | {{message}}",
    level="{log_level}",
    serialize={str(output_format == 'json').lower()}
)

# æ·»åŠ æ–‡ä»¶è¾“å‡º
logger.add(
    "logs/app.log",
    rotation="1 day",
    retention="30 days",
    level="{log_level}",
    format="{{time:YYYY-MM-DD HH:mm:ss}} | {{level}} | {{name}}:{{function}}:{{line}} | {{message}}"
)
```

ğŸ“Š æ—¥å¿—èšåˆ (ELK Stack):
```yaml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
```

ğŸ” æ—¥å¿—åˆ†ææ¨¡å¼:
- è¯·æ±‚è¿½è¸ª: åŒ…å«è¯·æ±‚IDçš„å®Œæ•´è¯·æ±‚é“¾è·¯
- é”™è¯¯èšåˆ: è‡ªåŠ¨åˆ†ç»„å’Œç»Ÿè®¡é”™è¯¯ç±»å‹
- æ€§èƒ½åˆ†æ: æ…¢æŸ¥è¯¢å’Œæ€§èƒ½ç“¶é¢ˆè¯†åˆ«
- å®‰å…¨å®¡è®¡: ç™»å½•ã€æƒé™å˜æ›´ç­‰å®‰å…¨äº‹ä»¶
"""
        return logging_config

    def _create_alert_rules(self, alert_types: List[str]) -> str:
        """åˆ›å»ºå‘Šè­¦è§„åˆ™"""
        alert_rules = [
            "ğŸš¨ ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®:",
            "",
            "âš¡ Prometheus AlertManagerè§„åˆ™:"
        ]
        
        rule_templates = {
            "high_error_rate": """
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "é«˜é”™è¯¯ç‡å‘Šè­¦"
      description: "åº”ç”¨é”™è¯¯ç‡è¶…è¿‡10%ï¼ŒæŒç»­2åˆ†é’Ÿ"
""",
            "high_response_time": """
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "å“åº”æ—¶é—´è¿‡é•¿"
      description: "95%åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡1ç§’ï¼ŒæŒç»­5åˆ†é’Ÿ"
""",
            "database_connection_high": """
  - alert: DatabaseConnectionHigh
    expr: database_connections_active / database_connections_max > 0.8
    for: 3m
    labels:
      severity: critical
    annotations:
      summary: "æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡è¿‡é«˜"
      description: "æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡è¶…è¿‡80%ï¼ŒæŒç»­3åˆ†é’Ÿ"
""",
            "memory_usage_high": """
  - alert: MemoryUsageHigh
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%ï¼ŒæŒç»­5åˆ†é’Ÿ"
"""
        }
        
        for alert_type in alert_types:
            if alert_type in rule_templates:
                alert_rules.append(rule_templates[alert_type])
        
        alert_rules.extend([
            "",
            "ğŸ“± é€šçŸ¥é…ç½®:",
            "- Slack: å®æ—¶å‘Šè­¦é€šçŸ¥",
            "- é‚®ä»¶: é‡è¦å‘Šè­¦æ±‡æ€»",
            "- é’‰é’‰: å›½å†…å›¢é˜Ÿé€šçŸ¥",
            "- PagerDuty: 7x24å€¼ç­å“åº”"
        ])
        
        return "\n".join(alert_rules)

    def _setup_monitoring_dashboard(self, dashboard_type: str, services: List[str]) -> str:
        """è®¾ç½®ç›‘æ§ä»ªè¡¨æ¿"""
        if dashboard_type == "grafana":
            dashboard_config = """
ğŸ“Š Grafanaä»ªè¡¨æ¿é…ç½®:

ğŸ¯ ä¸»è¦ä»ªè¡¨æ¿:

1. **åº”ç”¨æ¦‚è§ˆä»ªè¡¨æ¿**:
   - è¯·æ±‚é‡å’Œå“åº”æ—¶é—´è¶‹åŠ¿
   - é”™è¯¯ç‡å’ŒæˆåŠŸç‡ç»Ÿè®¡
   - æ´»è·ƒç”¨æˆ·å’Œä¼šè¯æ•°
   - ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

2. **æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿**:
   - APIç«¯ç‚¹æ€§èƒ½æ’è¡Œ
   - æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½åˆ†æ
   - ç¼“å­˜å‘½ä¸­ç‡å’Œæ•ˆæœ
   - ç³»ç»Ÿç“¶é¢ˆè¯†åˆ«

3. **åŸºç¡€è®¾æ–½ä»ªè¡¨æ¿**:
   - æœåŠ¡å™¨èµ„æºç›‘æ§
   - å®¹å™¨çŠ¶æ€å’Œèµ„æºä½¿ç”¨
   - ç½‘ç»œæµé‡å’Œå»¶è¿Ÿ
   - ç£ç›˜ä½¿ç”¨å’ŒI/Oæ€§èƒ½

4. **ä¸šåŠ¡ç›‘æ§ä»ªè¡¨æ¿**:
   - ç”¨æˆ·æ³¨å†Œå’Œç™»å½•è¶‹åŠ¿
   - åŠŸèƒ½ä½¿ç”¨ç»Ÿè®¡
   - ä¸šåŠ¡è½¬åŒ–ç‡åˆ†æ
   - å¼‚å¸¸è¡Œä¸ºæ£€æµ‹

ğŸ“ˆ å…³é”®æŒ‡æ ‡é¢æ¿:
```json
{
  "dashboard": {
    "title": "FastAPI Application Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Response Time P95",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      }
    ]
  }
}
```

ğŸ”§ è‡ªåŠ¨åŒ–é…ç½®:
- é€šè¿‡ä»£ç å®šä¹‰ä»ªè¡¨æ¿é…ç½®
- ç‰ˆæœ¬æ§åˆ¶ä»ªè¡¨æ¿å˜æ›´
- è‡ªåŠ¨åŒ–å¯¼å…¥å’Œæ›´æ–°
- ç¯å¢ƒé—´é…ç½®åŒæ­¥
"""
        else:
            dashboard_config = f"ğŸ“Š {dashboard_type} ä»ªè¡¨æ¿é…ç½® (å¾…å®ç°)"

        return dashboard_config